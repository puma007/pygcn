{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_scGCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfGuAGG9KcP4",
        "outputId": "9b07a7d5-e885-4c9c-881c-1bec3401b383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pygcn'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tkipf/pygcn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "V75bORgbSaaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/pygcn/pygcn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9f5oqhvKuYQ",
        "outputId": "8fca26ee-d053-4859-de83-94258d09308e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pygcn/pygcn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UodoUzW6K0SG",
        "outputId": "0ddfbbf4-f9c7-4a1d-ef0d-3bfb9a43a201"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pygcn/pygcn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   修改train.py 代码from pygcn.utils import load_data, accuracy 为 from utils import load_data, accuracy\n",
        "*   修改train.py 代码from pygcn.models import GCN 为 from models import GCN\n",
        "*   修改models.py 代码 from pygcn.layers import GraphConvolution 为from layers import GraphConvolution\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9Ve_hBWjvk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 上传由!Rscript data_preprocess.R生成的input文件夹，现将其压缩为input.tgz 文件\n",
        "!tar -xvf input.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdwuZmIifN5f",
        "outputId": "df174948-1f5b-412b-ebf5-605a8f9c45cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./._input\n",
            "input/\n",
            "input/._Label1.csv\n",
            "input/Label1.csv\n",
            "input/._inter_graph.csv\n",
            "input/inter_graph.csv\n",
            "input/._Label2.csv\n",
            "input/Label2.csv\n",
            "input/._Data2.csv\n",
            "input/Data2.csv\n",
            "input/._Data1.csv\n",
            "input/Data1.csv\n",
            "input/._intra_graph.csv\n",
            "input/intra_graph.csv\n",
            "input/._.ipynb_checkpoints\n",
            "input/.ipynb_checkpoints/\n",
            "input/._datasets.dat\n",
            "input/datasets.dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试修改后的代码是否正常运行\n",
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vg8th6jK8vU",
        "outputId": "95d869e3-5f56-44ad-9cf2-8ed3872e1f21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9418 acc_train: 0.1500 loss_val: 1.9129 acc_val: 0.3500 time: 0.4096s\n",
            "Epoch: 0002 loss_train: 1.9260 acc_train: 0.2857 loss_val: 1.9014 acc_val: 0.3500 time: 0.0137s\n",
            "Epoch: 0003 loss_train: 1.9181 acc_train: 0.3000 loss_val: 1.8900 acc_val: 0.3500 time: 0.0128s\n",
            "Epoch: 0004 loss_train: 1.9014 acc_train: 0.2929 loss_val: 1.8786 acc_val: 0.3500 time: 0.0137s\n",
            "Epoch: 0005 loss_train: 1.8925 acc_train: 0.2929 loss_val: 1.8672 acc_val: 0.3500 time: 0.0211s\n",
            "Epoch: 0006 loss_train: 1.8762 acc_train: 0.2929 loss_val: 1.8560 acc_val: 0.3500 time: 0.0157s\n",
            "Epoch: 0007 loss_train: 1.8709 acc_train: 0.2929 loss_val: 1.8451 acc_val: 0.3500 time: 0.0132s\n",
            "Epoch: 0008 loss_train: 1.8604 acc_train: 0.2929 loss_val: 1.8347 acc_val: 0.3500 time: 0.0097s\n",
            "Epoch: 0009 loss_train: 1.8415 acc_train: 0.2929 loss_val: 1.8248 acc_val: 0.3500 time: 0.0106s\n",
            "Epoch: 0010 loss_train: 1.8284 acc_train: 0.2929 loss_val: 1.8153 acc_val: 0.3500 time: 0.0088s\n",
            "Epoch: 0011 loss_train: 1.8299 acc_train: 0.2929 loss_val: 1.8064 acc_val: 0.3500 time: 0.0089s\n",
            "Epoch: 0012 loss_train: 1.8015 acc_train: 0.2929 loss_val: 1.7977 acc_val: 0.3500 time: 0.0096s\n",
            "Epoch: 0013 loss_train: 1.8103 acc_train: 0.2929 loss_val: 1.7896 acc_val: 0.3500 time: 0.0089s\n",
            "Epoch: 0014 loss_train: 1.8013 acc_train: 0.2929 loss_val: 1.7818 acc_val: 0.3500 time: 0.0086s\n",
            "Epoch: 0015 loss_train: 1.7759 acc_train: 0.2929 loss_val: 1.7743 acc_val: 0.3500 time: 0.0089s\n",
            "Epoch: 0016 loss_train: 1.7738 acc_train: 0.2929 loss_val: 1.7673 acc_val: 0.3500 time: 0.0090s\n",
            "Epoch: 0017 loss_train: 1.7545 acc_train: 0.2929 loss_val: 1.7606 acc_val: 0.3500 time: 0.0075s\n",
            "Epoch: 0018 loss_train: 1.7663 acc_train: 0.2929 loss_val: 1.7541 acc_val: 0.3500 time: 0.0074s\n",
            "Epoch: 0019 loss_train: 1.7519 acc_train: 0.2929 loss_val: 1.7477 acc_val: 0.3500 time: 0.0072s\n",
            "Epoch: 0020 loss_train: 1.7354 acc_train: 0.2929 loss_val: 1.7415 acc_val: 0.3500 time: 0.0073s\n",
            "Epoch: 0021 loss_train: 1.7456 acc_train: 0.2929 loss_val: 1.7354 acc_val: 0.3500 time: 0.0079s\n",
            "Epoch: 0022 loss_train: 1.7338 acc_train: 0.3000 loss_val: 1.7294 acc_val: 0.3500 time: 0.0072s\n",
            "Epoch: 0023 loss_train: 1.7267 acc_train: 0.3071 loss_val: 1.7235 acc_val: 0.3500 time: 0.0073s\n",
            "Epoch: 0024 loss_train: 1.6941 acc_train: 0.3143 loss_val: 1.7176 acc_val: 0.3500 time: 0.0073s\n",
            "Epoch: 0025 loss_train: 1.7308 acc_train: 0.3000 loss_val: 1.7116 acc_val: 0.3500 time: 0.0099s\n",
            "Epoch: 0026 loss_train: 1.6892 acc_train: 0.3000 loss_val: 1.7054 acc_val: 0.3500 time: 0.0101s\n",
            "Epoch: 0027 loss_train: 1.6894 acc_train: 0.3000 loss_val: 1.6991 acc_val: 0.3500 time: 0.0077s\n",
            "Epoch: 0028 loss_train: 1.6816 acc_train: 0.3071 loss_val: 1.6926 acc_val: 0.3500 time: 0.0075s\n",
            "Epoch: 0029 loss_train: 1.6506 acc_train: 0.3357 loss_val: 1.6860 acc_val: 0.3500 time: 0.0075s\n",
            "Epoch: 0030 loss_train: 1.6478 acc_train: 0.3214 loss_val: 1.6793 acc_val: 0.3533 time: 0.0075s\n",
            "Epoch: 0031 loss_train: 1.6456 acc_train: 0.3571 loss_val: 1.6725 acc_val: 0.3533 time: 0.0075s\n",
            "Epoch: 0032 loss_train: 1.6227 acc_train: 0.3714 loss_val: 1.6656 acc_val: 0.3600 time: 0.0084s\n",
            "Epoch: 0033 loss_train: 1.6341 acc_train: 0.3714 loss_val: 1.6584 acc_val: 0.3633 time: 0.0099s\n",
            "Epoch: 0034 loss_train: 1.6392 acc_train: 0.3571 loss_val: 1.6509 acc_val: 0.3667 time: 0.0093s\n",
            "Epoch: 0035 loss_train: 1.5981 acc_train: 0.3786 loss_val: 1.6430 acc_val: 0.3667 time: 0.0079s\n",
            "Epoch: 0036 loss_train: 1.6224 acc_train: 0.3857 loss_val: 1.6351 acc_val: 0.3700 time: 0.0071s\n",
            "Epoch: 0037 loss_train: 1.5988 acc_train: 0.4071 loss_val: 1.6268 acc_val: 0.3733 time: 0.0072s\n",
            "Epoch: 0038 loss_train: 1.5921 acc_train: 0.3786 loss_val: 1.6185 acc_val: 0.3767 time: 0.0073s\n",
            "Epoch: 0039 loss_train: 1.5610 acc_train: 0.4214 loss_val: 1.6097 acc_val: 0.3833 time: 0.0069s\n",
            "Epoch: 0040 loss_train: 1.5497 acc_train: 0.4500 loss_val: 1.6004 acc_val: 0.3867 time: 0.0069s\n",
            "Epoch: 0041 loss_train: 1.5569 acc_train: 0.4357 loss_val: 1.5905 acc_val: 0.3867 time: 0.0069s\n",
            "Epoch: 0042 loss_train: 1.5289 acc_train: 0.4286 loss_val: 1.5803 acc_val: 0.3900 time: 0.0071s\n",
            "Epoch: 0043 loss_train: 1.5246 acc_train: 0.4357 loss_val: 1.5700 acc_val: 0.3933 time: 0.0070s\n",
            "Epoch: 0044 loss_train: 1.4852 acc_train: 0.4357 loss_val: 1.5595 acc_val: 0.3967 time: 0.0071s\n",
            "Epoch: 0045 loss_train: 1.4765 acc_train: 0.4214 loss_val: 1.5490 acc_val: 0.4033 time: 0.0068s\n",
            "Epoch: 0046 loss_train: 1.4490 acc_train: 0.4571 loss_val: 1.5383 acc_val: 0.4100 time: 0.0070s\n",
            "Epoch: 0047 loss_train: 1.4501 acc_train: 0.4357 loss_val: 1.5276 acc_val: 0.4100 time: 0.0069s\n",
            "Epoch: 0048 loss_train: 1.4257 acc_train: 0.4571 loss_val: 1.5169 acc_val: 0.4133 time: 0.0071s\n",
            "Epoch: 0049 loss_train: 1.4382 acc_train: 0.4643 loss_val: 1.5061 acc_val: 0.4200 time: 0.0073s\n",
            "Epoch: 0050 loss_train: 1.4186 acc_train: 0.4571 loss_val: 1.4951 acc_val: 0.4267 time: 0.0069s\n",
            "Epoch: 0051 loss_train: 1.4013 acc_train: 0.4786 loss_val: 1.4838 acc_val: 0.4367 time: 0.0069s\n",
            "Epoch: 0052 loss_train: 1.4098 acc_train: 0.5214 loss_val: 1.4725 acc_val: 0.4600 time: 0.0105s\n",
            "Epoch: 0053 loss_train: 1.3632 acc_train: 0.5071 loss_val: 1.4612 acc_val: 0.4700 time: 0.0071s\n",
            "Epoch: 0054 loss_train: 1.3321 acc_train: 0.5429 loss_val: 1.4500 acc_val: 0.4867 time: 0.0076s\n",
            "Epoch: 0055 loss_train: 1.3358 acc_train: 0.5500 loss_val: 1.4389 acc_val: 0.5100 time: 0.0067s\n",
            "Epoch: 0056 loss_train: 1.3658 acc_train: 0.5143 loss_val: 1.4274 acc_val: 0.5133 time: 0.0068s\n",
            "Epoch: 0057 loss_train: 1.3080 acc_train: 0.5500 loss_val: 1.4159 acc_val: 0.5333 time: 0.0066s\n",
            "Epoch: 0058 loss_train: 1.2894 acc_train: 0.5786 loss_val: 1.4042 acc_val: 0.5400 time: 0.0067s\n",
            "Epoch: 0059 loss_train: 1.2883 acc_train: 0.6214 loss_val: 1.3925 acc_val: 0.5500 time: 0.0067s\n",
            "Epoch: 0060 loss_train: 1.2576 acc_train: 0.6143 loss_val: 1.3808 acc_val: 0.5600 time: 0.0074s\n",
            "Epoch: 0061 loss_train: 1.2735 acc_train: 0.5571 loss_val: 1.3692 acc_val: 0.5767 time: 0.0069s\n",
            "Epoch: 0062 loss_train: 1.2275 acc_train: 0.5929 loss_val: 1.3575 acc_val: 0.5867 time: 0.0067s\n",
            "Epoch: 0063 loss_train: 1.2039 acc_train: 0.6500 loss_val: 1.3459 acc_val: 0.6033 time: 0.0067s\n",
            "Epoch: 0064 loss_train: 1.2059 acc_train: 0.6571 loss_val: 1.3342 acc_val: 0.6200 time: 0.0066s\n",
            "Epoch: 0065 loss_train: 1.2011 acc_train: 0.6929 loss_val: 1.3226 acc_val: 0.6433 time: 0.0065s\n",
            "Epoch: 0066 loss_train: 1.1908 acc_train: 0.6500 loss_val: 1.3109 acc_val: 0.6600 time: 0.0069s\n",
            "Epoch: 0067 loss_train: 1.1588 acc_train: 0.7500 loss_val: 1.2993 acc_val: 0.6633 time: 0.0066s\n",
            "Epoch: 0068 loss_train: 1.1757 acc_train: 0.7214 loss_val: 1.2875 acc_val: 0.6733 time: 0.0066s\n",
            "Epoch: 0069 loss_train: 1.1578 acc_train: 0.7071 loss_val: 1.2756 acc_val: 0.6733 time: 0.0066s\n",
            "Epoch: 0070 loss_train: 1.1523 acc_train: 0.7214 loss_val: 1.2637 acc_val: 0.6700 time: 0.0066s\n",
            "Epoch: 0071 loss_train: 1.1280 acc_train: 0.7214 loss_val: 1.2520 acc_val: 0.6767 time: 0.0066s\n",
            "Epoch: 0072 loss_train: 1.1100 acc_train: 0.7000 loss_val: 1.2411 acc_val: 0.6767 time: 0.0078s\n",
            "Epoch: 0073 loss_train: 1.1220 acc_train: 0.7357 loss_val: 1.2302 acc_val: 0.6833 time: 0.0066s\n",
            "Epoch: 0074 loss_train: 1.0836 acc_train: 0.7429 loss_val: 1.2195 acc_val: 0.6833 time: 0.0065s\n",
            "Epoch: 0075 loss_train: 1.0509 acc_train: 0.7571 loss_val: 1.2092 acc_val: 0.6867 time: 0.0065s\n",
            "Epoch: 0076 loss_train: 1.0547 acc_train: 0.7857 loss_val: 1.1991 acc_val: 0.7067 time: 0.0065s\n",
            "Epoch: 0077 loss_train: 1.0465 acc_train: 0.7500 loss_val: 1.1889 acc_val: 0.7267 time: 0.0064s\n",
            "Epoch: 0078 loss_train: 1.0305 acc_train: 0.7214 loss_val: 1.1791 acc_val: 0.7467 time: 0.0076s\n",
            "Epoch: 0079 loss_train: 1.0413 acc_train: 0.7643 loss_val: 1.1693 acc_val: 0.7533 time: 0.0062s\n",
            "Epoch: 0080 loss_train: 1.0106 acc_train: 0.8143 loss_val: 1.1598 acc_val: 0.7533 time: 0.0070s\n",
            "Epoch: 0081 loss_train: 0.9964 acc_train: 0.7571 loss_val: 1.1507 acc_val: 0.7667 time: 0.0069s\n",
            "Epoch: 0082 loss_train: 0.9899 acc_train: 0.8000 loss_val: 1.1407 acc_val: 0.7700 time: 0.0064s\n",
            "Epoch: 0083 loss_train: 0.9608 acc_train: 0.8429 loss_val: 1.1307 acc_val: 0.7733 time: 0.0063s\n",
            "Epoch: 0084 loss_train: 0.9542 acc_train: 0.8357 loss_val: 1.1208 acc_val: 0.7667 time: 0.0068s\n",
            "Epoch: 0085 loss_train: 0.9495 acc_train: 0.7857 loss_val: 1.1116 acc_val: 0.7633 time: 0.0081s\n",
            "Epoch: 0086 loss_train: 0.9413 acc_train: 0.7929 loss_val: 1.1030 acc_val: 0.7700 time: 0.0066s\n",
            "Epoch: 0087 loss_train: 0.9663 acc_train: 0.7786 loss_val: 1.0944 acc_val: 0.7733 time: 0.0062s\n",
            "Epoch: 0088 loss_train: 0.9113 acc_train: 0.8357 loss_val: 1.0865 acc_val: 0.7800 time: 0.0063s\n",
            "Epoch: 0089 loss_train: 0.8868 acc_train: 0.8357 loss_val: 1.0787 acc_val: 0.7767 time: 0.0065s\n",
            "Epoch: 0090 loss_train: 0.9040 acc_train: 0.8143 loss_val: 1.0706 acc_val: 0.7800 time: 0.0067s\n",
            "Epoch: 0091 loss_train: 0.9029 acc_train: 0.8143 loss_val: 1.0623 acc_val: 0.7800 time: 0.0066s\n",
            "Epoch: 0092 loss_train: 0.8879 acc_train: 0.8214 loss_val: 1.0542 acc_val: 0.7800 time: 0.0078s\n",
            "Epoch: 0093 loss_train: 0.8104 acc_train: 0.8286 loss_val: 1.0467 acc_val: 0.7800 time: 0.0073s\n",
            "Epoch: 0094 loss_train: 0.8451 acc_train: 0.8357 loss_val: 1.0385 acc_val: 0.7833 time: 0.0067s\n",
            "Epoch: 0095 loss_train: 0.8474 acc_train: 0.8214 loss_val: 1.0309 acc_val: 0.7867 time: 0.0077s\n",
            "Epoch: 0096 loss_train: 0.8215 acc_train: 0.8643 loss_val: 1.0240 acc_val: 0.7800 time: 0.0091s\n",
            "Epoch: 0097 loss_train: 0.8042 acc_train: 0.8643 loss_val: 1.0166 acc_val: 0.7900 time: 0.0066s\n",
            "Epoch: 0098 loss_train: 0.8214 acc_train: 0.8500 loss_val: 1.0102 acc_val: 0.7833 time: 0.0067s\n",
            "Epoch: 0099 loss_train: 0.7911 acc_train: 0.8714 loss_val: 1.0038 acc_val: 0.7900 time: 0.0065s\n",
            "Epoch: 0100 loss_train: 0.7995 acc_train: 0.8429 loss_val: 0.9971 acc_val: 0.7900 time: 0.0065s\n",
            "Epoch: 0101 loss_train: 0.7862 acc_train: 0.8571 loss_val: 0.9906 acc_val: 0.7900 time: 0.0067s\n",
            "Epoch: 0102 loss_train: 0.7843 acc_train: 0.8571 loss_val: 0.9834 acc_val: 0.7967 time: 0.0066s\n",
            "Epoch: 0103 loss_train: 0.7655 acc_train: 0.8500 loss_val: 0.9766 acc_val: 0.7933 time: 0.0065s\n",
            "Epoch: 0104 loss_train: 0.7490 acc_train: 0.8500 loss_val: 0.9695 acc_val: 0.7900 time: 0.0064s\n",
            "Epoch: 0105 loss_train: 0.7371 acc_train: 0.8857 loss_val: 0.9628 acc_val: 0.7833 time: 0.0063s\n",
            "Epoch: 0106 loss_train: 0.7638 acc_train: 0.8429 loss_val: 0.9562 acc_val: 0.7833 time: 0.0064s\n",
            "Epoch: 0107 loss_train: 0.7236 acc_train: 0.8500 loss_val: 0.9501 acc_val: 0.7867 time: 0.0064s\n",
            "Epoch: 0108 loss_train: 0.7140 acc_train: 0.8714 loss_val: 0.9442 acc_val: 0.7900 time: 0.0065s\n",
            "Epoch: 0109 loss_train: 0.7337 acc_train: 0.8429 loss_val: 0.9389 acc_val: 0.7867 time: 0.0066s\n",
            "Epoch: 0110 loss_train: 0.7380 acc_train: 0.9000 loss_val: 0.9333 acc_val: 0.7867 time: 0.0074s\n",
            "Epoch: 0111 loss_train: 0.7281 acc_train: 0.8571 loss_val: 0.9275 acc_val: 0.7867 time: 0.0065s\n",
            "Epoch: 0112 loss_train: 0.7257 acc_train: 0.8357 loss_val: 0.9206 acc_val: 0.7833 time: 0.0064s\n",
            "Epoch: 0113 loss_train: 0.6837 acc_train: 0.8571 loss_val: 0.9132 acc_val: 0.7933 time: 0.0064s\n",
            "Epoch: 0114 loss_train: 0.7101 acc_train: 0.8857 loss_val: 0.9066 acc_val: 0.7967 time: 0.0063s\n",
            "Epoch: 0115 loss_train: 0.6910 acc_train: 0.8714 loss_val: 0.9002 acc_val: 0.7933 time: 0.0064s\n",
            "Epoch: 0116 loss_train: 0.6730 acc_train: 0.8786 loss_val: 0.8943 acc_val: 0.7900 time: 0.0065s\n",
            "Epoch: 0117 loss_train: 0.6608 acc_train: 0.8357 loss_val: 0.8892 acc_val: 0.7933 time: 0.0065s\n",
            "Epoch: 0118 loss_train: 0.6567 acc_train: 0.8929 loss_val: 0.8845 acc_val: 0.7933 time: 0.0064s\n",
            "Epoch: 0119 loss_train: 0.6401 acc_train: 0.8786 loss_val: 0.8805 acc_val: 0.7933 time: 0.0065s\n",
            "Epoch: 0120 loss_train: 0.6616 acc_train: 0.8786 loss_val: 0.8773 acc_val: 0.7900 time: 0.0063s\n",
            "Epoch: 0121 loss_train: 0.6750 acc_train: 0.8714 loss_val: 0.8743 acc_val: 0.7900 time: 0.0067s\n",
            "Epoch: 0122 loss_train: 0.6224 acc_train: 0.9143 loss_val: 0.8703 acc_val: 0.7933 time: 0.0066s\n",
            "Epoch: 0123 loss_train: 0.6232 acc_train: 0.8786 loss_val: 0.8656 acc_val: 0.7933 time: 0.0065s\n",
            "Epoch: 0124 loss_train: 0.6400 acc_train: 0.8714 loss_val: 0.8605 acc_val: 0.7933 time: 0.0067s\n",
            "Epoch: 0125 loss_train: 0.6584 acc_train: 0.8786 loss_val: 0.8556 acc_val: 0.7967 time: 0.0068s\n",
            "Epoch: 0126 loss_train: 0.6286 acc_train: 0.8571 loss_val: 0.8508 acc_val: 0.7967 time: 0.0066s\n",
            "Epoch: 0127 loss_train: 0.6053 acc_train: 0.8714 loss_val: 0.8458 acc_val: 0.7933 time: 0.0066s\n",
            "Epoch: 0128 loss_train: 0.5824 acc_train: 0.9286 loss_val: 0.8408 acc_val: 0.7933 time: 0.0084s\n",
            "Epoch: 0129 loss_train: 0.5979 acc_train: 0.8714 loss_val: 0.8367 acc_val: 0.7967 time: 0.0095s\n",
            "Epoch: 0130 loss_train: 0.5819 acc_train: 0.8929 loss_val: 0.8328 acc_val: 0.8000 time: 0.0072s\n",
            "Epoch: 0131 loss_train: 0.5647 acc_train: 0.9071 loss_val: 0.8289 acc_val: 0.8033 time: 0.0065s\n",
            "Epoch: 0132 loss_train: 0.6387 acc_train: 0.8714 loss_val: 0.8253 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0133 loss_train: 0.6101 acc_train: 0.9214 loss_val: 0.8213 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0134 loss_train: 0.5823 acc_train: 0.8786 loss_val: 0.8165 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0135 loss_train: 0.5846 acc_train: 0.8857 loss_val: 0.8128 acc_val: 0.8100 time: 0.0065s\n",
            "Epoch: 0136 loss_train: 0.5763 acc_train: 0.8714 loss_val: 0.8101 acc_val: 0.8067 time: 0.0066s\n",
            "Epoch: 0137 loss_train: 0.5410 acc_train: 0.9000 loss_val: 0.8070 acc_val: 0.8067 time: 0.0066s\n",
            "Epoch: 0138 loss_train: 0.5914 acc_train: 0.8857 loss_val: 0.8039 acc_val: 0.8033 time: 0.0065s\n",
            "Epoch: 0139 loss_train: 0.5638 acc_train: 0.9143 loss_val: 0.8003 acc_val: 0.8067 time: 0.0065s\n",
            "Epoch: 0140 loss_train: 0.5519 acc_train: 0.9071 loss_val: 0.7974 acc_val: 0.8067 time: 0.0096s\n",
            "Epoch: 0141 loss_train: 0.5737 acc_train: 0.8929 loss_val: 0.7946 acc_val: 0.8067 time: 0.0065s\n",
            "Epoch: 0142 loss_train: 0.5359 acc_train: 0.8714 loss_val: 0.7922 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0143 loss_train: 0.5002 acc_train: 0.9286 loss_val: 0.7899 acc_val: 0.8033 time: 0.0065s\n",
            "Epoch: 0144 loss_train: 0.5621 acc_train: 0.9000 loss_val: 0.7874 acc_val: 0.8033 time: 0.0065s\n",
            "Epoch: 0145 loss_train: 0.5549 acc_train: 0.8857 loss_val: 0.7843 acc_val: 0.8033 time: 0.0064s\n",
            "Epoch: 0146 loss_train: 0.5566 acc_train: 0.8929 loss_val: 0.7810 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0147 loss_train: 0.5560 acc_train: 0.8857 loss_val: 0.7787 acc_val: 0.8067 time: 0.0088s\n",
            "Epoch: 0148 loss_train: 0.5182 acc_train: 0.9214 loss_val: 0.7770 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0149 loss_train: 0.5052 acc_train: 0.9143 loss_val: 0.7755 acc_val: 0.8100 time: 0.0064s\n",
            "Epoch: 0150 loss_train: 0.5317 acc_train: 0.9214 loss_val: 0.7740 acc_val: 0.8067 time: 0.0062s\n",
            "Epoch: 0151 loss_train: 0.5322 acc_train: 0.8786 loss_val: 0.7720 acc_val: 0.8100 time: 0.0063s\n",
            "Epoch: 0152 loss_train: 0.5067 acc_train: 0.9071 loss_val: 0.7702 acc_val: 0.8100 time: 0.0063s\n",
            "Epoch: 0153 loss_train: 0.5178 acc_train: 0.9000 loss_val: 0.7685 acc_val: 0.8067 time: 0.0066s\n",
            "Epoch: 0154 loss_train: 0.5560 acc_train: 0.8929 loss_val: 0.7664 acc_val: 0.8067 time: 0.0067s\n",
            "Epoch: 0155 loss_train: 0.5070 acc_train: 0.9143 loss_val: 0.7636 acc_val: 0.8067 time: 0.0067s\n",
            "Epoch: 0156 loss_train: 0.4655 acc_train: 0.9429 loss_val: 0.7603 acc_val: 0.8067 time: 0.0065s\n",
            "Epoch: 0157 loss_train: 0.5066 acc_train: 0.9286 loss_val: 0.7569 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0158 loss_train: 0.4829 acc_train: 0.9143 loss_val: 0.7544 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0159 loss_train: 0.5092 acc_train: 0.8929 loss_val: 0.7521 acc_val: 0.8067 time: 0.0064s\n",
            "Epoch: 0160 loss_train: 0.5267 acc_train: 0.8857 loss_val: 0.7505 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0161 loss_train: 0.5139 acc_train: 0.9071 loss_val: 0.7496 acc_val: 0.8033 time: 0.0062s\n",
            "Epoch: 0162 loss_train: 0.5064 acc_train: 0.9071 loss_val: 0.7485 acc_val: 0.8067 time: 0.0063s\n",
            "Epoch: 0163 loss_train: 0.5038 acc_train: 0.9143 loss_val: 0.7464 acc_val: 0.8033 time: 0.0063s\n",
            "Epoch: 0164 loss_train: 0.4925 acc_train: 0.9429 loss_val: 0.7433 acc_val: 0.8033 time: 0.0063s\n",
            "Epoch: 0165 loss_train: 0.5175 acc_train: 0.8857 loss_val: 0.7413 acc_val: 0.8033 time: 0.0062s\n",
            "Epoch: 0166 loss_train: 0.5059 acc_train: 0.9071 loss_val: 0.7402 acc_val: 0.8067 time: 0.0062s\n",
            "Epoch: 0167 loss_train: 0.4761 acc_train: 0.9286 loss_val: 0.7388 acc_val: 0.8033 time: 0.0063s\n",
            "Epoch: 0168 loss_train: 0.4873 acc_train: 0.9071 loss_val: 0.7372 acc_val: 0.8000 time: 0.0064s\n",
            "Epoch: 0169 loss_train: 0.4752 acc_train: 0.9143 loss_val: 0.7357 acc_val: 0.8000 time: 0.0064s\n",
            "Epoch: 0170 loss_train: 0.5031 acc_train: 0.9071 loss_val: 0.7339 acc_val: 0.8033 time: 0.0070s\n",
            "Epoch: 0171 loss_train: 0.4349 acc_train: 0.9071 loss_val: 0.7319 acc_val: 0.8000 time: 0.0095s\n",
            "Epoch: 0172 loss_train: 0.4696 acc_train: 0.9000 loss_val: 0.7295 acc_val: 0.8067 time: 0.0067s\n",
            "Epoch: 0173 loss_train: 0.4626 acc_train: 0.9286 loss_val: 0.7273 acc_val: 0.8100 time: 0.0066s\n",
            "Epoch: 0174 loss_train: 0.4609 acc_train: 0.9214 loss_val: 0.7258 acc_val: 0.8133 time: 0.0064s\n",
            "Epoch: 0175 loss_train: 0.4454 acc_train: 0.9214 loss_val: 0.7248 acc_val: 0.8133 time: 0.0063s\n",
            "Epoch: 0176 loss_train: 0.4583 acc_train: 0.9357 loss_val: 0.7243 acc_val: 0.8167 time: 0.0064s\n",
            "Epoch: 0177 loss_train: 0.4822 acc_train: 0.9500 loss_val: 0.7227 acc_val: 0.8133 time: 0.0062s\n",
            "Epoch: 0178 loss_train: 0.4393 acc_train: 0.9214 loss_val: 0.7213 acc_val: 0.8133 time: 0.0065s\n",
            "Epoch: 0179 loss_train: 0.4550 acc_train: 0.9500 loss_val: 0.7200 acc_val: 0.8133 time: 0.0062s\n",
            "Epoch: 0180 loss_train: 0.4744 acc_train: 0.9286 loss_val: 0.7183 acc_val: 0.8133 time: 0.0077s\n",
            "Epoch: 0181 loss_train: 0.4847 acc_train: 0.8786 loss_val: 0.7157 acc_val: 0.8133 time: 0.0123s\n",
            "Epoch: 0182 loss_train: 0.4441 acc_train: 0.9071 loss_val: 0.7140 acc_val: 0.8167 time: 0.0092s\n",
            "Epoch: 0183 loss_train: 0.4261 acc_train: 0.9357 loss_val: 0.7129 acc_val: 0.8133 time: 0.0069s\n",
            "Epoch: 0184 loss_train: 0.4492 acc_train: 0.9286 loss_val: 0.7121 acc_val: 0.8167 time: 0.0069s\n",
            "Epoch: 0185 loss_train: 0.4773 acc_train: 0.9286 loss_val: 0.7110 acc_val: 0.8200 time: 0.0064s\n",
            "Epoch: 0186 loss_train: 0.4365 acc_train: 0.9429 loss_val: 0.7094 acc_val: 0.8233 time: 0.0062s\n",
            "Epoch: 0187 loss_train: 0.4769 acc_train: 0.9214 loss_val: 0.7072 acc_val: 0.8167 time: 0.0066s\n",
            "Epoch: 0188 loss_train: 0.4523 acc_train: 0.9357 loss_val: 0.7051 acc_val: 0.8200 time: 0.0063s\n",
            "Epoch: 0189 loss_train: 0.4279 acc_train: 0.9429 loss_val: 0.7028 acc_val: 0.8267 time: 0.0065s\n",
            "Epoch: 0190 loss_train: 0.4674 acc_train: 0.9214 loss_val: 0.7020 acc_val: 0.8267 time: 0.0062s\n",
            "Epoch: 0191 loss_train: 0.4567 acc_train: 0.9357 loss_val: 0.7016 acc_val: 0.8267 time: 0.0064s\n",
            "Epoch: 0192 loss_train: 0.4190 acc_train: 0.9214 loss_val: 0.7010 acc_val: 0.8267 time: 0.0063s\n",
            "Epoch: 0193 loss_train: 0.4327 acc_train: 0.9143 loss_val: 0.7000 acc_val: 0.8200 time: 0.0062s\n",
            "Epoch: 0194 loss_train: 0.4393 acc_train: 0.8929 loss_val: 0.6986 acc_val: 0.8167 time: 0.0064s\n",
            "Epoch: 0195 loss_train: 0.4135 acc_train: 0.9357 loss_val: 0.6977 acc_val: 0.8133 time: 0.0065s\n",
            "Epoch: 0196 loss_train: 0.3838 acc_train: 0.9571 loss_val: 0.6980 acc_val: 0.8033 time: 0.0065s\n",
            "Epoch: 0197 loss_train: 0.4049 acc_train: 0.9500 loss_val: 0.6980 acc_val: 0.7967 time: 0.0065s\n",
            "Epoch: 0198 loss_train: 0.4667 acc_train: 0.9286 loss_val: 0.6968 acc_val: 0.7933 time: 0.0064s\n",
            "Epoch: 0199 loss_train: 0.4348 acc_train: 0.9071 loss_val: 0.6947 acc_val: 0.8033 time: 0.0069s\n",
            "Epoch: 0200 loss_train: 0.4478 acc_train: 0.9214 loss_val: 0.6936 acc_val: 0.8167 time: 0.0065s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.9038s\n",
            "Test set results: loss= 0.7374 accuracy= 0.8270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   上传data，data删除from graph\n",
        "*   上传utils2\n",
        "*   注释utils2的81，82行，将features转成稀疏矩阵即可，修改为\n",
        "*   #features = sp.vstack((sp.vstack((datas_tr, datas_va)), datas_te)).tolil()\n",
        "*   #features = preprocess_features(features)\n",
        "*   修改为features = sp.vstack((sp.vstack((datas_tr, datas_va)), datas_te))\n",
        "\n"
      ],
      "metadata": {
        "id": "EK9ab3HbKpS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "from utils2 import *\n",
        "adj, features, labels_binary_train, labels_binary_val, labels_binary_test, train_mask, pred_mask, val_mask, test_mask, new_label, true_label, index_guide = load_data(\n",
        "    datadir='input',rgraph=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AwF5OWOLNd7",
        "outputId": "2df05bff-d0c2-4a33-8def-8f2d8e7caaf8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "load data succesfully....\n",
            "assign input coordinatly....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax_SWPfPfL7y",
        "outputId": "02758fd4-8348-4a54-edc2-dfd1bbc90b49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9105x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 3523159 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "metadata": {
        "id": "Ymy_sYE5gYv3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = normalize(features)"
      ],
      "metadata": {
        "id": "WQhr7GlZgb3L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "features = torch.FloatTensor(np.array(features.todense()))"
      ],
      "metadata": {
        "id": "ZpQyraozgjv3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuGJz8IYgq8T",
        "outputId": "4ff45224-02e7-4beb-ebc4-b3c0e4b54c5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0004],\n",
              "        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0005, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0005]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nnTogx2iPUYt",
        "outputId": "d45e01bd-5a16-4341-e742-425ceca3814b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e4cdffb7-885c-42f7-b1f8-6d65623510aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>alpha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>alpha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alpha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alpha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9100</th>\n",
              "      <td>quiescent_stellate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9101</th>\n",
              "      <td>quiescent_stellate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9102</th>\n",
              "      <td>quiescent_stellate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9103</th>\n",
              "      <td>quiescent_stellate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9104</th>\n",
              "      <td>quiescent_stellate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9105 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4cdffb7-885c-42f7-b1f8-6d65623510aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e4cdffb7-885c-42f7-b1f8-6d65623510aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e4cdffb7-885c-42f7-b1f8-6d65623510aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       0\n",
              "0                  alpha\n",
              "1                  alpha\n",
              "2                  alpha\n",
              "3                  alpha\n",
              "4                  alpha\n",
              "...                  ...\n",
              "9100  quiescent_stellate\n",
              "9101  quiescent_stellate\n",
              "9102  quiescent_stellate\n",
              "9103  quiescent_stellate\n",
              "9104  quiescent_stellate\n",
              "\n",
              "[9105 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_guide"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNhbyOTPyaj",
        "outputId": "e3022799-de4e-4207-9101-336069fc57a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 137, 1582, 1392, ...,  108,  480,  382])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_binary_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtZhxZaeP-Of",
        "outputId": "faf6edb9-98da-4752-dd17-c4ddf7178c23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ],
      "metadata": {
        "id": "MrnRnwW-QGws"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot"
      ],
      "metadata": {
        "id": "tx5Ej193QcTW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = encode_onehot(true_label[0])"
      ],
      "metadata": {
        "id": "d77S2rn6QhAa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL3Y0o-3QpwR",
        "outputId": "882226f0-c570-44ac-dd33-2db3bb9adec6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVIR2PK3QrrC",
        "outputId": "7abc4b79-22c7-4fdc-e352-34375456c440"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPxE-ebNQwG9",
        "outputId": "7bda8f12-3cec-41c1-c2c0-ed44325ab5f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9105"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#features = torch.FloatTensor(np.array(features.todense()))\n",
        "#返回值为1的索引位置\n",
        "#np.where()[0] 表示行的索引，\n",
        "#np.where()[1] 则表示列的索引\n",
        "labels = torch.LongTensor(np.where(labels)[1])\n",
        "#adj = sparse_mx_to_torch_sparse_tensor(adj)"
      ],
      "metadata": {
        "id": "lm5RQjwTRiNT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[9102]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiOkz58cSTXb",
        "outputId": "00b90916-c6d6-414b-ff84-a3cd9f5db089"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_train = range(1483)\n",
        "idx_val = range(8747, 8917)\n",
        "idx_test = range(8917, 9105)\n",
        "idx_pred= range(1483, 8747)"
      ],
      "metadata": {
        "id": "GZXJFkZkSjne"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from utils import load_data, accuracy\n",
        "from models import GCN"
      ],
      "metadata": {
        "id": "C9Ciq4d1Ub0e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "KwnkORGOVGD6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "0ztDZTS_VP-M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z-FHsstVZf9",
        "outputId": "2286a56e-6ad6-4b97-fc78-a2556f8b44c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9105x9105 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 24063 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM8Tr5dIVdHz",
        "outputId": "4121ef87-53be-4f53-eaa1-5351dad6d42a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 4, 4,  ..., 6, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPnqZNiUVfmC",
        "outputId": "c1293ba9-715d-4c3f-cadd-2c9c3d9ecaff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 1483)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FrkrDtWGQi",
        "outputId": "9d1db2cd-7104-4586-8769-44c0138c8244"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dropout'], dest='dropout', nargs=None, const=None, default=0.5, type=<class 'float'>, choices=None, help='Dropout rate (1 - keep probability).', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=0.01, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "T4l5uMP_Vh7b"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tn0PC97Vmi-",
        "outputId": "aa8120fe-3206-450e-a82e-693bc7831db6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9105x9105 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 24063 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "6gBw-r8eZHC2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj = sparse_mx_to_torch_sparse_tensor(adj)"
      ],
      "metadata": {
        "id": "6loNbj-6cUHQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TmxaVv3cYjL",
        "outputId": "b859a2fc-f6c3-4219-ef12-d6d0a8bbf754"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[   0,    1,    2,  ..., 3057, 4554, 9104],\n",
              "                       [   0,    1,    2,  ..., 9104, 9104, 9104]]),\n",
              "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
              "       size=(9105, 9105), nnz=24063, layout=torch.sparse_coo)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ],
      "metadata": {
        "id": "PRlRLqyZcZ59"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))"
      ],
      "metadata": {
        "id": "AyOdMiZohdG7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOarH-LhfqA",
        "outputId": "0ed3921e-9614-437e-df9e-363ebf7ac0cc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 2.3538 acc_train: 0.0883 loss_val: 2.3628 acc_val: 0.0706 time: 0.1363s\n",
            "Epoch: 0002 loss_train: 2.2643 acc_train: 0.1126 loss_val: 2.2791 acc_val: 0.1235 time: 0.0434s\n",
            "Epoch: 0003 loss_train: 2.1691 acc_train: 0.1436 loss_val: 2.1694 acc_val: 0.1294 time: 0.0406s\n",
            "Epoch: 0004 loss_train: 2.0856 acc_train: 0.1740 loss_val: 2.0785 acc_val: 0.1824 time: 0.0419s\n",
            "Epoch: 0005 loss_train: 2.0067 acc_train: 0.1854 loss_val: 1.9906 acc_val: 0.1882 time: 0.0457s\n",
            "Epoch: 0006 loss_train: 1.9394 acc_train: 0.2293 loss_val: 1.9437 acc_val: 0.2235 time: 0.0438s\n",
            "Epoch: 0007 loss_train: 1.8641 acc_train: 0.2994 loss_val: 1.8819 acc_val: 0.3235 time: 0.0406s\n",
            "Epoch: 0008 loss_train: 1.7899 acc_train: 0.3796 loss_val: 1.8075 acc_val: 0.3235 time: 0.0423s\n",
            "Epoch: 0009 loss_train: 1.7129 acc_train: 0.4174 loss_val: 1.7323 acc_val: 0.4294 time: 0.0414s\n",
            "Epoch: 0010 loss_train: 1.6406 acc_train: 0.4659 loss_val: 1.6707 acc_val: 0.4529 time: 0.0422s\n",
            "Epoch: 0011 loss_train: 1.5732 acc_train: 0.5024 loss_val: 1.5816 acc_val: 0.5588 time: 0.0419s\n",
            "Epoch: 0012 loss_train: 1.4936 acc_train: 0.5779 loss_val: 1.5252 acc_val: 0.5529 time: 0.0407s\n",
            "Epoch: 0013 loss_train: 1.4349 acc_train: 0.5974 loss_val: 1.4399 acc_val: 0.6000 time: 0.0412s\n",
            "Epoch: 0014 loss_train: 1.3942 acc_train: 0.6264 loss_val: 1.4158 acc_val: 0.6235 time: 0.0419s\n",
            "Epoch: 0015 loss_train: 1.3252 acc_train: 0.6601 loss_val: 1.3390 acc_val: 0.6353 time: 0.0445s\n",
            "Epoch: 0016 loss_train: 1.2691 acc_train: 0.6837 loss_val: 1.2928 acc_val: 0.6353 time: 0.0464s\n",
            "Epoch: 0017 loss_train: 1.2451 acc_train: 0.6709 loss_val: 1.2764 acc_val: 0.6647 time: 0.0498s\n",
            "Epoch: 0018 loss_train: 1.2043 acc_train: 0.6891 loss_val: 1.2264 acc_val: 0.6824 time: 0.0399s\n",
            "Epoch: 0019 loss_train: 1.1575 acc_train: 0.7006 loss_val: 1.1936 acc_val: 0.6941 time: 0.0398s\n",
            "Epoch: 0020 loss_train: 1.1391 acc_train: 0.7114 loss_val: 1.1913 acc_val: 0.6941 time: 0.0430s\n",
            "Epoch: 0021 loss_train: 1.0858 acc_train: 0.7202 loss_val: 1.0628 acc_val: 0.7529 time: 0.0441s\n",
            "Epoch: 0022 loss_train: 1.0813 acc_train: 0.7208 loss_val: 1.0658 acc_val: 0.7235 time: 0.0409s\n",
            "Epoch: 0023 loss_train: 1.0389 acc_train: 0.7323 loss_val: 1.1017 acc_val: 0.6824 time: 0.0465s\n",
            "Epoch: 0024 loss_train: 1.0066 acc_train: 0.7390 loss_val: 1.0060 acc_val: 0.7824 time: 0.0417s\n",
            "Epoch: 0025 loss_train: 0.9791 acc_train: 0.7579 loss_val: 1.0097 acc_val: 0.7176 time: 0.0406s\n",
            "Epoch: 0026 loss_train: 0.9597 acc_train: 0.7478 loss_val: 0.9863 acc_val: 0.7353 time: 0.0450s\n",
            "Epoch: 0027 loss_train: 0.9294 acc_train: 0.7586 loss_val: 0.9300 acc_val: 0.7353 time: 0.0443s\n",
            "Epoch: 0028 loss_train: 0.9109 acc_train: 0.7593 loss_val: 0.8585 acc_val: 0.7941 time: 0.0423s\n",
            "Epoch: 0029 loss_train: 0.8810 acc_train: 0.7748 loss_val: 0.8960 acc_val: 0.7765 time: 0.0463s\n",
            "Epoch: 0030 loss_train: 0.8731 acc_train: 0.7734 loss_val: 0.8240 acc_val: 0.8176 time: 0.0465s\n",
            "Epoch: 0031 loss_train: 0.8526 acc_train: 0.7674 loss_val: 0.8269 acc_val: 0.7706 time: 0.0479s\n",
            "Epoch: 0032 loss_train: 0.8415 acc_train: 0.7782 loss_val: 0.8596 acc_val: 0.7882 time: 0.0443s\n",
            "Epoch: 0033 loss_train: 0.8194 acc_train: 0.7795 loss_val: 0.8249 acc_val: 0.7882 time: 0.0457s\n",
            "Epoch: 0034 loss_train: 0.8099 acc_train: 0.7883 loss_val: 0.8469 acc_val: 0.7529 time: 0.0434s\n",
            "Epoch: 0035 loss_train: 0.8125 acc_train: 0.7788 loss_val: 0.7996 acc_val: 0.7941 time: 0.0428s\n",
            "Epoch: 0036 loss_train: 0.7837 acc_train: 0.7869 loss_val: 0.7843 acc_val: 0.8059 time: 0.0464s\n",
            "Epoch: 0037 loss_train: 0.7770 acc_train: 0.7923 loss_val: 0.7970 acc_val: 0.7765 time: 0.0430s\n",
            "Epoch: 0038 loss_train: 0.7754 acc_train: 0.7930 loss_val: 0.8009 acc_val: 0.7941 time: 0.0454s\n",
            "Epoch: 0039 loss_train: 0.7713 acc_train: 0.7802 loss_val: 0.6793 acc_val: 0.8294 time: 0.0401s\n",
            "Epoch: 0040 loss_train: 0.7457 acc_train: 0.7862 loss_val: 0.7463 acc_val: 0.7941 time: 0.0524s\n",
            "Epoch: 0041 loss_train: 0.7472 acc_train: 0.7862 loss_val: 0.7094 acc_val: 0.8294 time: 0.0440s\n",
            "Epoch: 0042 loss_train: 0.7268 acc_train: 0.8078 loss_val: 0.7230 acc_val: 0.8176 time: 0.0446s\n",
            "Epoch: 0043 loss_train: 0.7163 acc_train: 0.8024 loss_val: 0.6983 acc_val: 0.8000 time: 0.0451s\n",
            "Epoch: 0044 loss_train: 0.7094 acc_train: 0.7970 loss_val: 0.6875 acc_val: 0.8294 time: 0.0452s\n",
            "Epoch: 0045 loss_train: 0.7107 acc_train: 0.7997 loss_val: 0.6695 acc_val: 0.8588 time: 0.0446s\n",
            "Epoch: 0046 loss_train: 0.7129 acc_train: 0.8018 loss_val: 0.6617 acc_val: 0.8118 time: 0.0472s\n",
            "Epoch: 0047 loss_train: 0.6974 acc_train: 0.8024 loss_val: 0.6924 acc_val: 0.7941 time: 0.0474s\n",
            "Epoch: 0048 loss_train: 0.6786 acc_train: 0.8193 loss_val: 0.6886 acc_val: 0.8294 time: 0.0420s\n",
            "Epoch: 0049 loss_train: 0.6879 acc_train: 0.8098 loss_val: 0.7221 acc_val: 0.7941 time: 0.0399s\n",
            "Epoch: 0050 loss_train: 0.6585 acc_train: 0.8186 loss_val: 0.6675 acc_val: 0.8294 time: 0.0402s\n",
            "Epoch: 0051 loss_train: 0.6721 acc_train: 0.8254 loss_val: 0.6715 acc_val: 0.8059 time: 0.0454s\n",
            "Epoch: 0052 loss_train: 0.6588 acc_train: 0.8213 loss_val: 0.6370 acc_val: 0.8529 time: 0.0453s\n",
            "Epoch: 0053 loss_train: 0.6734 acc_train: 0.8119 loss_val: 0.6709 acc_val: 0.8529 time: 0.0447s\n",
            "Epoch: 0054 loss_train: 0.6584 acc_train: 0.8260 loss_val: 0.6662 acc_val: 0.8294 time: 0.0497s\n",
            "Epoch: 0055 loss_train: 0.6766 acc_train: 0.8146 loss_val: 0.6272 acc_val: 0.8059 time: 0.0412s\n",
            "Epoch: 0056 loss_train: 0.6449 acc_train: 0.8220 loss_val: 0.6493 acc_val: 0.8353 time: 0.0427s\n",
            "Epoch: 0057 loss_train: 0.6459 acc_train: 0.8098 loss_val: 0.5579 acc_val: 0.8706 time: 0.0407s\n",
            "Epoch: 0058 loss_train: 0.6203 acc_train: 0.8294 loss_val: 0.6391 acc_val: 0.7941 time: 0.0415s\n",
            "Epoch: 0059 loss_train: 0.6351 acc_train: 0.8240 loss_val: 0.6385 acc_val: 0.8118 time: 0.0419s\n",
            "Epoch: 0060 loss_train: 0.6385 acc_train: 0.8240 loss_val: 0.6285 acc_val: 0.8353 time: 0.0424s\n",
            "Epoch: 0061 loss_train: 0.6133 acc_train: 0.8361 loss_val: 0.6286 acc_val: 0.8353 time: 0.0478s\n",
            "Epoch: 0062 loss_train: 0.6277 acc_train: 0.8186 loss_val: 0.6356 acc_val: 0.8118 time: 0.0479s\n",
            "Epoch: 0063 loss_train: 0.6185 acc_train: 0.8206 loss_val: 0.6588 acc_val: 0.8176 time: 0.0413s\n",
            "Epoch: 0064 loss_train: 0.6116 acc_train: 0.8294 loss_val: 0.6516 acc_val: 0.8294 time: 0.0406s\n",
            "Epoch: 0065 loss_train: 0.6344 acc_train: 0.8294 loss_val: 0.6091 acc_val: 0.8235 time: 0.0450s\n",
            "Epoch: 0066 loss_train: 0.6107 acc_train: 0.8395 loss_val: 0.6032 acc_val: 0.8588 time: 0.0436s\n",
            "Epoch: 0067 loss_train: 0.6105 acc_train: 0.8260 loss_val: 0.5910 acc_val: 0.8529 time: 0.0435s\n",
            "Epoch: 0068 loss_train: 0.6002 acc_train: 0.8328 loss_val: 0.5736 acc_val: 0.8588 time: 0.0447s\n",
            "Epoch: 0069 loss_train: 0.5880 acc_train: 0.8355 loss_val: 0.6150 acc_val: 0.8118 time: 0.0426s\n",
            "Epoch: 0070 loss_train: 0.5970 acc_train: 0.8395 loss_val: 0.6390 acc_val: 0.8412 time: 0.0434s\n",
            "Epoch: 0071 loss_train: 0.5859 acc_train: 0.8422 loss_val: 0.6235 acc_val: 0.8471 time: 0.0437s\n",
            "Epoch: 0072 loss_train: 0.5915 acc_train: 0.8341 loss_val: 0.5790 acc_val: 0.8353 time: 0.0436s\n",
            "Epoch: 0073 loss_train: 0.5744 acc_train: 0.8348 loss_val: 0.5967 acc_val: 0.8529 time: 0.0416s\n",
            "Epoch: 0074 loss_train: 0.6096 acc_train: 0.8240 loss_val: 0.5455 acc_val: 0.8529 time: 0.0422s\n",
            "Epoch: 0075 loss_train: 0.5781 acc_train: 0.8510 loss_val: 0.5757 acc_val: 0.8294 time: 0.0427s\n",
            "Epoch: 0076 loss_train: 0.5840 acc_train: 0.8348 loss_val: 0.6233 acc_val: 0.8588 time: 0.0447s\n",
            "Epoch: 0077 loss_train: 0.5713 acc_train: 0.8490 loss_val: 0.5408 acc_val: 0.8647 time: 0.0441s\n",
            "Epoch: 0078 loss_train: 0.5788 acc_train: 0.8469 loss_val: 0.5704 acc_val: 0.8647 time: 0.0413s\n",
            "Epoch: 0079 loss_train: 0.5774 acc_train: 0.8382 loss_val: 0.5645 acc_val: 0.8471 time: 0.0397s\n",
            "Epoch: 0080 loss_train: 0.5626 acc_train: 0.8483 loss_val: 0.5465 acc_val: 0.8588 time: 0.0411s\n",
            "Epoch: 0081 loss_train: 0.5868 acc_train: 0.8361 loss_val: 0.5401 acc_val: 0.8588 time: 0.0439s\n",
            "Epoch: 0082 loss_train: 0.5658 acc_train: 0.8415 loss_val: 0.6071 acc_val: 0.8588 time: 0.0399s\n",
            "Epoch: 0083 loss_train: 0.5576 acc_train: 0.8422 loss_val: 0.5684 acc_val: 0.8412 time: 0.0398s\n",
            "Epoch: 0084 loss_train: 0.5715 acc_train: 0.8436 loss_val: 0.5561 acc_val: 0.8529 time: 0.0439s\n",
            "Epoch: 0085 loss_train: 0.5688 acc_train: 0.8415 loss_val: 0.5565 acc_val: 0.8765 time: 0.0412s\n",
            "Epoch: 0086 loss_train: 0.5367 acc_train: 0.8456 loss_val: 0.5415 acc_val: 0.8647 time: 0.0508s\n",
            "Epoch: 0087 loss_train: 0.5406 acc_train: 0.8564 loss_val: 0.5597 acc_val: 0.8529 time: 0.0394s\n",
            "Epoch: 0088 loss_train: 0.5531 acc_train: 0.8415 loss_val: 0.5792 acc_val: 0.8647 time: 0.0439s\n",
            "Epoch: 0089 loss_train: 0.5492 acc_train: 0.8503 loss_val: 0.5551 acc_val: 0.8647 time: 0.0418s\n",
            "Epoch: 0090 loss_train: 0.5396 acc_train: 0.8429 loss_val: 0.5307 acc_val: 0.8647 time: 0.0407s\n",
            "Epoch: 0091 loss_train: 0.5493 acc_train: 0.8456 loss_val: 0.4874 acc_val: 0.8588 time: 0.0453s\n",
            "Epoch: 0092 loss_train: 0.5305 acc_train: 0.8550 loss_val: 0.5610 acc_val: 0.8588 time: 0.0413s\n",
            "Epoch: 0093 loss_train: 0.5368 acc_train: 0.8550 loss_val: 0.5807 acc_val: 0.8471 time: 0.0446s\n",
            "Epoch: 0094 loss_train: 0.5462 acc_train: 0.8456 loss_val: 0.5535 acc_val: 0.8824 time: 0.0428s\n",
            "Epoch: 0095 loss_train: 0.5278 acc_train: 0.8537 loss_val: 0.5732 acc_val: 0.8706 time: 0.0403s\n",
            "Epoch: 0096 loss_train: 0.5265 acc_train: 0.8611 loss_val: 0.5431 acc_val: 0.8412 time: 0.0419s\n",
            "Epoch: 0097 loss_train: 0.5363 acc_train: 0.8537 loss_val: 0.4656 acc_val: 0.8824 time: 0.0413s\n",
            "Epoch: 0098 loss_train: 0.5398 acc_train: 0.8449 loss_val: 0.5508 acc_val: 0.8647 time: 0.0423s\n",
            "Epoch: 0099 loss_train: 0.5354 acc_train: 0.8584 loss_val: 0.4833 acc_val: 0.8529 time: 0.0429s\n",
            "Epoch: 0100 loss_train: 0.5177 acc_train: 0.8611 loss_val: 0.5068 acc_val: 0.8882 time: 0.0433s\n",
            "Epoch: 0101 loss_train: 0.5182 acc_train: 0.8483 loss_val: 0.4491 acc_val: 0.8529 time: 0.0457s\n",
            "Epoch: 0102 loss_train: 0.5189 acc_train: 0.8577 loss_val: 0.5439 acc_val: 0.8647 time: 0.0393s\n",
            "Epoch: 0103 loss_train: 0.4985 acc_train: 0.8597 loss_val: 0.5096 acc_val: 0.8588 time: 0.0426s\n",
            "Epoch: 0104 loss_train: 0.4926 acc_train: 0.8705 loss_val: 0.4831 acc_val: 0.8647 time: 0.0412s\n",
            "Epoch: 0105 loss_train: 0.5225 acc_train: 0.8496 loss_val: 0.5315 acc_val: 0.8235 time: 0.0435s\n",
            "Epoch: 0106 loss_train: 0.4884 acc_train: 0.8672 loss_val: 0.5106 acc_val: 0.8824 time: 0.0444s\n",
            "Epoch: 0107 loss_train: 0.5026 acc_train: 0.8638 loss_val: 0.4901 acc_val: 0.8588 time: 0.0385s\n",
            "Epoch: 0108 loss_train: 0.5170 acc_train: 0.8604 loss_val: 0.4418 acc_val: 0.8941 time: 0.0396s\n",
            "Epoch: 0109 loss_train: 0.5043 acc_train: 0.8557 loss_val: 0.5433 acc_val: 0.8647 time: 0.0474s\n",
            "Epoch: 0110 loss_train: 0.4893 acc_train: 0.8624 loss_val: 0.5096 acc_val: 0.8588 time: 0.0391s\n",
            "Epoch: 0111 loss_train: 0.5021 acc_train: 0.8651 loss_val: 0.5075 acc_val: 0.8824 time: 0.0443s\n",
            "Epoch: 0112 loss_train: 0.4947 acc_train: 0.8550 loss_val: 0.5111 acc_val: 0.8765 time: 0.0431s\n",
            "Epoch: 0113 loss_train: 0.4845 acc_train: 0.8712 loss_val: 0.5161 acc_val: 0.9118 time: 0.0414s\n",
            "Epoch: 0114 loss_train: 0.4745 acc_train: 0.8780 loss_val: 0.4815 acc_val: 0.8765 time: 0.0402s\n",
            "Epoch: 0115 loss_train: 0.4781 acc_train: 0.8726 loss_val: 0.5150 acc_val: 0.8706 time: 0.0412s\n",
            "Epoch: 0116 loss_train: 0.4848 acc_train: 0.8618 loss_val: 0.5146 acc_val: 0.8765 time: 0.0427s\n",
            "Epoch: 0117 loss_train: 0.4749 acc_train: 0.8624 loss_val: 0.4706 acc_val: 0.8941 time: 0.0449s\n",
            "Epoch: 0118 loss_train: 0.4806 acc_train: 0.8726 loss_val: 0.4721 acc_val: 0.8647 time: 0.0438s\n",
            "Epoch: 0119 loss_train: 0.4821 acc_train: 0.8550 loss_val: 0.4545 acc_val: 0.8824 time: 0.0398s\n",
            "Epoch: 0120 loss_train: 0.4767 acc_train: 0.8732 loss_val: 0.4936 acc_val: 0.8588 time: 0.0442s\n",
            "Epoch: 0121 loss_train: 0.4737 acc_train: 0.8678 loss_val: 0.4734 acc_val: 0.8882 time: 0.0495s\n",
            "Epoch: 0122 loss_train: 0.4590 acc_train: 0.8739 loss_val: 0.4316 acc_val: 0.8882 time: 0.0430s\n",
            "Epoch: 0123 loss_train: 0.4813 acc_train: 0.8624 loss_val: 0.5108 acc_val: 0.8471 time: 0.0431s\n",
            "Epoch: 0124 loss_train: 0.4815 acc_train: 0.8719 loss_val: 0.4764 acc_val: 0.8765 time: 0.0401s\n",
            "Epoch: 0125 loss_train: 0.4786 acc_train: 0.8726 loss_val: 0.5173 acc_val: 0.8882 time: 0.0461s\n",
            "Epoch: 0126 loss_train: 0.4696 acc_train: 0.8712 loss_val: 0.4585 acc_val: 0.8882 time: 0.0453s\n",
            "Epoch: 0127 loss_train: 0.4475 acc_train: 0.8847 loss_val: 0.5477 acc_val: 0.8941 time: 0.0449s\n",
            "Epoch: 0128 loss_train: 0.4523 acc_train: 0.8732 loss_val: 0.4677 acc_val: 0.8824 time: 0.0447s\n",
            "Epoch: 0129 loss_train: 0.4685 acc_train: 0.8806 loss_val: 0.4700 acc_val: 0.8647 time: 0.0416s\n",
            "Epoch: 0130 loss_train: 0.4628 acc_train: 0.8732 loss_val: 0.4710 acc_val: 0.8765 time: 0.0405s\n",
            "Epoch: 0131 loss_train: 0.4416 acc_train: 0.8840 loss_val: 0.4340 acc_val: 0.8941 time: 0.0452s\n",
            "Epoch: 0132 loss_train: 0.4720 acc_train: 0.8746 loss_val: 0.4837 acc_val: 0.8471 time: 0.0485s\n",
            "Epoch: 0133 loss_train: 0.4633 acc_train: 0.8732 loss_val: 0.4304 acc_val: 0.8824 time: 0.0399s\n",
            "Epoch: 0134 loss_train: 0.4658 acc_train: 0.8726 loss_val: 0.4288 acc_val: 0.9000 time: 0.0416s\n",
            "Epoch: 0135 loss_train: 0.4512 acc_train: 0.8847 loss_val: 0.4909 acc_val: 0.9000 time: 0.0386s\n",
            "Epoch: 0136 loss_train: 0.4631 acc_train: 0.8753 loss_val: 0.4024 acc_val: 0.8882 time: 0.0434s\n",
            "Epoch: 0137 loss_train: 0.4622 acc_train: 0.8638 loss_val: 0.3582 acc_val: 0.9000 time: 0.0480s\n",
            "Epoch: 0138 loss_train: 0.4469 acc_train: 0.8739 loss_val: 0.4941 acc_val: 0.8824 time: 0.0403s\n",
            "Epoch: 0139 loss_train: 0.4406 acc_train: 0.8780 loss_val: 0.3926 acc_val: 0.8941 time: 0.0437s\n",
            "Epoch: 0140 loss_train: 0.4334 acc_train: 0.8793 loss_val: 0.4518 acc_val: 0.9000 time: 0.0403s\n",
            "Epoch: 0141 loss_train: 0.4465 acc_train: 0.8833 loss_val: 0.4819 acc_val: 0.8824 time: 0.0479s\n",
            "Epoch: 0142 loss_train: 0.4425 acc_train: 0.8766 loss_val: 0.5272 acc_val: 0.8765 time: 0.0401s\n",
            "Epoch: 0143 loss_train: 0.4500 acc_train: 0.8746 loss_val: 0.3848 acc_val: 0.9059 time: 0.0405s\n",
            "Epoch: 0144 loss_train: 0.4505 acc_train: 0.8806 loss_val: 0.4328 acc_val: 0.8882 time: 0.0396s\n",
            "Epoch: 0145 loss_train: 0.4374 acc_train: 0.8793 loss_val: 0.4360 acc_val: 0.8765 time: 0.0448s\n",
            "Epoch: 0146 loss_train: 0.4239 acc_train: 0.8881 loss_val: 0.4533 acc_val: 0.8765 time: 0.0456s\n",
            "Epoch: 0147 loss_train: 0.4403 acc_train: 0.8813 loss_val: 0.5118 acc_val: 0.9059 time: 0.0401s\n",
            "Epoch: 0148 loss_train: 0.4298 acc_train: 0.8867 loss_val: 0.3916 acc_val: 0.9000 time: 0.0391s\n",
            "Epoch: 0149 loss_train: 0.4366 acc_train: 0.8699 loss_val: 0.5215 acc_val: 0.8765 time: 0.0412s\n",
            "Epoch: 0150 loss_train: 0.4353 acc_train: 0.8887 loss_val: 0.4496 acc_val: 0.9176 time: 0.0421s\n",
            "Epoch: 0151 loss_train: 0.4473 acc_train: 0.8638 loss_val: 0.3677 acc_val: 0.9118 time: 0.0447s\n",
            "Epoch: 0152 loss_train: 0.4447 acc_train: 0.8860 loss_val: 0.4716 acc_val: 0.9000 time: 0.0407s\n",
            "Epoch: 0153 loss_train: 0.4228 acc_train: 0.8881 loss_val: 0.4105 acc_val: 0.9176 time: 0.0414s\n",
            "Epoch: 0154 loss_train: 0.4298 acc_train: 0.8847 loss_val: 0.4758 acc_val: 0.8588 time: 0.0456s\n",
            "Epoch: 0155 loss_train: 0.4375 acc_train: 0.8753 loss_val: 0.3985 acc_val: 0.8647 time: 0.0492s\n",
            "Epoch: 0156 loss_train: 0.4345 acc_train: 0.8881 loss_val: 0.3978 acc_val: 0.8588 time: 0.0450s\n",
            "Epoch: 0157 loss_train: 0.4370 acc_train: 0.8793 loss_val: 0.4502 acc_val: 0.9000 time: 0.0397s\n",
            "Epoch: 0158 loss_train: 0.4275 acc_train: 0.8840 loss_val: 0.4477 acc_val: 0.9235 time: 0.0416s\n",
            "Epoch: 0159 loss_train: 0.4161 acc_train: 0.8887 loss_val: 0.4018 acc_val: 0.9176 time: 0.0410s\n",
            "Epoch: 0160 loss_train: 0.4257 acc_train: 0.8780 loss_val: 0.4275 acc_val: 0.9000 time: 0.0418s\n",
            "Epoch: 0161 loss_train: 0.4214 acc_train: 0.8860 loss_val: 0.4332 acc_val: 0.9059 time: 0.0449s\n",
            "Epoch: 0162 loss_train: 0.4154 acc_train: 0.8820 loss_val: 0.3793 acc_val: 0.9294 time: 0.0419s\n",
            "Epoch: 0163 loss_train: 0.4193 acc_train: 0.8806 loss_val: 0.5227 acc_val: 0.8647 time: 0.0435s\n",
            "Epoch: 0164 loss_train: 0.4324 acc_train: 0.8813 loss_val: 0.4019 acc_val: 0.8824 time: 0.0412s\n",
            "Epoch: 0165 loss_train: 0.4290 acc_train: 0.8847 loss_val: 0.3715 acc_val: 0.8882 time: 0.0438s\n",
            "Epoch: 0166 loss_train: 0.4014 acc_train: 0.8881 loss_val: 0.4569 acc_val: 0.8765 time: 0.0448s\n",
            "Epoch: 0167 loss_train: 0.4017 acc_train: 0.8833 loss_val: 0.3664 acc_val: 0.8882 time: 0.0404s\n",
            "Epoch: 0168 loss_train: 0.4038 acc_train: 0.8800 loss_val: 0.3965 acc_val: 0.9000 time: 0.0421s\n",
            "Epoch: 0169 loss_train: 0.4230 acc_train: 0.8854 loss_val: 0.4273 acc_val: 0.9118 time: 0.0418s\n",
            "Epoch: 0170 loss_train: 0.4269 acc_train: 0.8847 loss_val: 0.3949 acc_val: 0.8941 time: 0.0427s\n",
            "Epoch: 0171 loss_train: 0.3977 acc_train: 0.8921 loss_val: 0.4120 acc_val: 0.8941 time: 0.0459s\n",
            "Epoch: 0172 loss_train: 0.4212 acc_train: 0.8786 loss_val: 0.4877 acc_val: 0.9000 time: 0.0401s\n",
            "Epoch: 0173 loss_train: 0.4108 acc_train: 0.8941 loss_val: 0.4492 acc_val: 0.9118 time: 0.0412s\n",
            "Epoch: 0174 loss_train: 0.4258 acc_train: 0.8901 loss_val: 0.4145 acc_val: 0.9118 time: 0.0430s\n",
            "Epoch: 0175 loss_train: 0.4317 acc_train: 0.8833 loss_val: 0.3940 acc_val: 0.8941 time: 0.0396s\n",
            "Epoch: 0176 loss_train: 0.4309 acc_train: 0.8833 loss_val: 0.4322 acc_val: 0.8882 time: 0.0433s\n",
            "Epoch: 0177 loss_train: 0.4177 acc_train: 0.8753 loss_val: 0.3855 acc_val: 0.9118 time: 0.0404s\n",
            "Epoch: 0178 loss_train: 0.4079 acc_train: 0.8833 loss_val: 0.4437 acc_val: 0.8941 time: 0.0411s\n",
            "Epoch: 0179 loss_train: 0.3969 acc_train: 0.8935 loss_val: 0.5032 acc_val: 0.9059 time: 0.0542s\n",
            "Epoch: 0180 loss_train: 0.4064 acc_train: 0.8948 loss_val: 0.4984 acc_val: 0.8824 time: 0.0407s\n",
            "Epoch: 0181 loss_train: 0.4065 acc_train: 0.8887 loss_val: 0.4101 acc_val: 0.9176 time: 0.0427s\n",
            "Epoch: 0182 loss_train: 0.4242 acc_train: 0.8827 loss_val: 0.4459 acc_val: 0.8765 time: 0.0397s\n",
            "Epoch: 0183 loss_train: 0.4137 acc_train: 0.8793 loss_val: 0.4955 acc_val: 0.9000 time: 0.0388s\n",
            "Epoch: 0184 loss_train: 0.3924 acc_train: 0.8901 loss_val: 0.4635 acc_val: 0.9235 time: 0.0390s\n",
            "Epoch: 0185 loss_train: 0.4097 acc_train: 0.9002 loss_val: 0.3629 acc_val: 0.9000 time: 0.0393s\n",
            "Epoch: 0186 loss_train: 0.3967 acc_train: 0.8914 loss_val: 0.3748 acc_val: 0.9059 time: 0.0399s\n",
            "Epoch: 0187 loss_train: 0.4092 acc_train: 0.8874 loss_val: 0.4560 acc_val: 0.9000 time: 0.0471s\n",
            "Epoch: 0188 loss_train: 0.4117 acc_train: 0.8786 loss_val: 0.3826 acc_val: 0.9059 time: 0.0407s\n",
            "Epoch: 0189 loss_train: 0.3984 acc_train: 0.8793 loss_val: 0.4899 acc_val: 0.9118 time: 0.0433s\n",
            "Epoch: 0190 loss_train: 0.4073 acc_train: 0.8901 loss_val: 0.4623 acc_val: 0.9000 time: 0.0417s\n",
            "Epoch: 0191 loss_train: 0.4010 acc_train: 0.8894 loss_val: 0.4619 acc_val: 0.9118 time: 0.0417s\n",
            "Epoch: 0192 loss_train: 0.4122 acc_train: 0.8901 loss_val: 0.4519 acc_val: 0.9118 time: 0.0456s\n",
            "Epoch: 0193 loss_train: 0.4044 acc_train: 0.8874 loss_val: 0.4666 acc_val: 0.8824 time: 0.0421s\n",
            "Epoch: 0194 loss_train: 0.3812 acc_train: 0.8867 loss_val: 0.4784 acc_val: 0.8882 time: 0.0424s\n",
            "Epoch: 0195 loss_train: 0.4013 acc_train: 0.8854 loss_val: 0.3938 acc_val: 0.9412 time: 0.0402s\n",
            "Epoch: 0196 loss_train: 0.3917 acc_train: 0.8901 loss_val: 0.3943 acc_val: 0.9000 time: 0.0454s\n",
            "Epoch: 0197 loss_train: 0.3968 acc_train: 0.8901 loss_val: 0.3744 acc_val: 0.9118 time: 0.0446s\n",
            "Epoch: 0198 loss_train: 0.3912 acc_train: 0.8867 loss_val: 0.4076 acc_val: 0.9294 time: 0.0394s\n",
            "Epoch: 0199 loss_train: 0.4114 acc_train: 0.8833 loss_val: 0.3970 acc_val: 0.8941 time: 0.0396s\n",
            "Epoch: 0200 loss_train: 0.4057 acc_train: 0.8833 loss_val: 0.4134 acc_val: 0.9059 time: 0.0419s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.9192s\n",
            "Test set results: loss= 0.2885 accuracy= 0.9415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVfRZN7Bhilc",
        "outputId": "9cf6b43d-0e24-4879-c109-5455ec989050"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf pygcn.tgz pygcn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yj_tgV2p1o2",
        "outputId": "0486fd09-0700-4077-ac38-fe6fad761083"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygcn/\n",
            "pygcn/README.md\n",
            "pygcn/LICENCE\n",
            "pygcn/setup.py\n",
            "pygcn/pygcn/\n",
            "pygcn/pygcn/__pycache__/\n",
            "pygcn/pygcn/__pycache__/utils.cpython-37.pyc\n",
            "pygcn/pygcn/__pycache__/utils2.cpython-37.pyc\n",
            "pygcn/pygcn/__pycache__/layers.cpython-37.pyc\n",
            "pygcn/pygcn/__pycache__/models.cpython-37.pyc\n",
            "pygcn/pygcn/__pycache__/data.cpython-37.pyc\n",
            "pygcn/pygcn/utils.py\n",
            "pygcn/pygcn/utils2.py\n",
            "pygcn/pygcn/data.py\n",
            "pygcn/pygcn/._input\n",
            "pygcn/pygcn/__init__.py\n",
            "pygcn/pygcn/models.py\n",
            "pygcn/pygcn/train.py\n",
            "pygcn/pygcn/input.tgz\n",
            "pygcn/pygcn/input/\n",
            "pygcn/pygcn/input/._inter_graph.csv\n",
            "pygcn/pygcn/input/._Label2.csv\n",
            "pygcn/pygcn/input/._Data1.csv\n",
            "pygcn/pygcn/input/inter_graph.csv\n",
            "pygcn/pygcn/input/Label1.csv\n",
            "pygcn/pygcn/input/._.ipynb_checkpoints\n",
            "pygcn/pygcn/input/._Label1.csv\n",
            "pygcn/pygcn/input/._intra_graph.csv\n",
            "pygcn/pygcn/input/Label2.csv\n",
            "pygcn/pygcn/input/._Data2.csv\n",
            "pygcn/pygcn/input/Data2.csv\n",
            "pygcn/pygcn/input/datasets.dat\n",
            "pygcn/pygcn/input/Data1.csv\n",
            "pygcn/pygcn/input/intra_graph.csv\n",
            "pygcn/pygcn/input/.ipynb_checkpoints/\n",
            "pygcn/pygcn/input/._datasets.dat\n",
            "pygcn/pygcn/layers.py\n",
            "pygcn/.gitignore\n",
            "pygcn/data/\n",
            "pygcn/data/cora/\n",
            "pygcn/data/cora/cora.content\n",
            "pygcn/data/cora/README\n",
            "pygcn/data/cora/cora.cites\n",
            "pygcn/.git/\n",
            "pygcn/.git/objects/\n",
            "pygcn/.git/objects/d9/\n",
            "pygcn/.git/objects/d9/71e9d07e5f439864ac216697287faad7eb4406\n",
            "pygcn/.git/objects/93/\n",
            "pygcn/.git/objects/93/68dd7ca842fa25ddeff436d3873eabb7c0b00c\n",
            "pygcn/.git/objects/bf/\n",
            "pygcn/.git/objects/bf/a83a015b9025ddbca2b7c1ed543c66fd3af3d9\n",
            "pygcn/.git/objects/bf/1410eb6462771803ea9b8a09b9391c286f06c2\n",
            "pygcn/.git/objects/bf/fa4b5482bfe7b5d77ed7c5d1b1f7b644a1c8d2\n",
            "pygcn/.git/objects/47/\n",
            "pygcn/.git/objects/47/8e77fedaa9845a891d0aff8c98cd7c43c331d9\n",
            "pygcn/.git/objects/fd/\n",
            "pygcn/.git/objects/fd/bf5dd5cbe81338539503016a89aa1e524a8275\n",
            "pygcn/.git/objects/fd/20fddf874731c364880a33eb9acd43c1512365\n",
            "pygcn/.git/objects/68/\n",
            "pygcn/.git/objects/68/7f2d4d84598d6ad1f1488e1e95b51c634a188c\n",
            "pygcn/.git/objects/ff/\n",
            "pygcn/.git/objects/ff/c697f49af91a75a91c22d29d3754d0e5dbdc8e\n",
            "pygcn/.git/objects/97/\n",
            "pygcn/.git/objects/97/6cd6d4b9a05b28078e554007dcf680eec19863\n",
            "pygcn/.git/objects/54/\n",
            "pygcn/.git/objects/54/88806187f2b91d8ac5777026bdf6a20ba35637\n",
            "pygcn/.git/objects/ce/\n",
            "pygcn/.git/objects/ce/f0a3c62dc7773df1fe162cea46f9ae1b2dc764\n",
            "pygcn/.git/objects/f5/\n",
            "pygcn/.git/objects/f5/ffee573108172f55d2a9ee808776f24cf1fc77\n",
            "pygcn/.git/objects/f5/06269b499e78b2ffd1e4a13c459252e4d08668\n",
            "pygcn/.git/objects/74/\n",
            "pygcn/.git/objects/74/65037d0ea3e86f8edfce9a1b6bdca026b76b9b\n",
            "pygcn/.git/objects/74/74897566c1b198f0b83b2cc91fce3db9f2d89e\n",
            "pygcn/.git/objects/27/\n",
            "pygcn/.git/objects/27/089c68f9a812fd0d15069581f875544c7992f7\n",
            "pygcn/.git/objects/76/\n",
            "pygcn/.git/objects/76/1af8c388bd5284beae2576f81a533db7e9490a\n",
            "pygcn/.git/objects/6f/\n",
            "pygcn/.git/objects/6f/661e0e3ea5788ffd5f6d3f1ef284e20f683b57\n",
            "pygcn/.git/objects/55/\n",
            "pygcn/.git/objects/55/6eecd9855c737d8c2ba7a79d3f2eaa66d4c6a5\n",
            "pygcn/.git/objects/c0/\n",
            "pygcn/.git/objects/c0/92c587ce2d5b746315d288617e801f8c093c93\n",
            "pygcn/.git/objects/3a/\n",
            "pygcn/.git/objects/3a/f13e96e846cc4831c04cf31b433700658de8d0\n",
            "pygcn/.git/objects/89/\n",
            "pygcn/.git/objects/89/b485ae78e50cb7bb4982a7844ed280a2e7bad4\n",
            "pygcn/.git/objects/ec/\n",
            "pygcn/.git/objects/ec/1b125e4552740a14e9259448827b7873b26c7e\n",
            "pygcn/.git/objects/ec/04fe44e533c25a25bc86e1f86b26e84f32d2be\n",
            "pygcn/.git/objects/ec/3c7012266136a06fb8ccad4e02333205c13dbd\n",
            "pygcn/.git/objects/2d/\n",
            "pygcn/.git/objects/2d/a4f9c2b5cfeaf2f620d2b14d0e5fc09119599f\n",
            "pygcn/.git/objects/91/\n",
            "pygcn/.git/objects/91/b2fcf5af5f868699f71d6e09235cbe2da7a06f\n",
            "pygcn/.git/objects/f0/\n",
            "pygcn/.git/objects/f0/65462de08aaaa9fef458f111b2e61e36e69a2c\n",
            "pygcn/.git/objects/8d/\n",
            "pygcn/.git/objects/8d/8e0bdfbed7bf7e9e8295f5264e47b1037135c9\n",
            "pygcn/.git/objects/17/\n",
            "pygcn/.git/objects/17/4cd9a5eef7632abe7cd776ff291a4245336753\n",
            "pygcn/.git/objects/c2/\n",
            "pygcn/.git/objects/c2/8e2b005d09d1d820a337a0c5a714118d53f111\n",
            "pygcn/.git/objects/49/\n",
            "pygcn/.git/objects/49/c94006bf376c88398b7939506a8c33dc61ede8\n",
            "pygcn/.git/objects/49/4ea71a7adbcd64002303301af3c04b514b329b\n",
            "pygcn/.git/objects/95/\n",
            "pygcn/.git/objects/95/6d87c1b7ea7c0a03331a947222ed54cb2e2765\n",
            "pygcn/.git/objects/b2/\n",
            "pygcn/.git/objects/b2/3cb9397f84990604b735cc03f2766f606e179a\n",
            "pygcn/.git/objects/dc/\n",
            "pygcn/.git/objects/dc/a2d47e7c0a3082be8cf06ea13d84af775d9507\n",
            "pygcn/.git/objects/62/\n",
            "pygcn/.git/objects/62/39adb7fa83da3a0ab800fa202ec089f82486d6\n",
            "pygcn/.git/objects/info/\n",
            "pygcn/.git/objects/87/\n",
            "pygcn/.git/objects/87/6eb89e9665d5ecfc23c163425e9e5d5656dd0b\n",
            "pygcn/.git/objects/f4/\n",
            "pygcn/.git/objects/f4/37c8c35495078a2154f91694004d7563bc29d1\n",
            "pygcn/.git/objects/88/\n",
            "pygcn/.git/objects/88/c6676b2ab98b04bf3bef96b46ea037ebb07b12\n",
            "pygcn/.git/objects/b4/\n",
            "pygcn/.git/objects/b4/855a7d38fbe483d6b3bf01a3dcc36494eeb655\n",
            "pygcn/.git/objects/9b/\n",
            "pygcn/.git/objects/9b/53c5bd78bcc384dc8ecfa1a662d753e6746aeb\n",
            "pygcn/.git/objects/9b/a169144605f0eeedfcc0f99b9dd14d16cfa289\n",
            "pygcn/.git/objects/a1/\n",
            "pygcn/.git/objects/a1/09fef174dba52fe88d81867ad4c494ae279988\n",
            "pygcn/.git/objects/71/\n",
            "pygcn/.git/objects/71/0e402b77d76cd00085f651bacabb23e8ac9d3d\n",
            "pygcn/.git/objects/5c/\n",
            "pygcn/.git/objects/5c/3ec21cf063e719e37ff62ce6839c5b2908ff47\n",
            "pygcn/.git/objects/b9/\n",
            "pygcn/.git/objects/b9/7a07573ce2ad1d644acb277d5865bee8756c16\n",
            "pygcn/.git/objects/c3/\n",
            "pygcn/.git/objects/c3/eb6f165c154bd97120941973cf25e785c6604d\n",
            "pygcn/.git/objects/09/\n",
            "pygcn/.git/objects/09/53726a3061f98b3a09574f990d0df7a47ce195\n",
            "pygcn/.git/objects/39/\n",
            "pygcn/.git/objects/39/99c5adb897490911e9c25eb592c9ea80c889a9\n",
            "pygcn/.git/objects/d3/\n",
            "pygcn/.git/objects/d3/e4f39f68ff90abb19ea3da54812574a4f8b151\n",
            "pygcn/.git/objects/f7/\n",
            "pygcn/.git/objects/f7/010295b0a6570744241f2184e3cfaf7aa83857\n",
            "pygcn/.git/objects/de/\n",
            "pygcn/.git/objects/de/d00c40b34466d04af337db824bbc5fc413adbc\n",
            "pygcn/.git/objects/0b/\n",
            "pygcn/.git/objects/0b/7251a145263087f08297e17c196b3c4ecb847a\n",
            "pygcn/.git/objects/c9/\n",
            "pygcn/.git/objects/c9/1f69c7d0c7fbd1a8fd2bafde9d1cb48b290c06\n",
            "pygcn/.git/objects/43/\n",
            "pygcn/.git/objects/43/96e4db5b97c9e071516bc601c9b6693696c489\n",
            "pygcn/.git/objects/cd/\n",
            "pygcn/.git/objects/cd/e584ad3d1b45923627c08f358ab517a3f9c9b0\n",
            "pygcn/.git/objects/a6/\n",
            "pygcn/.git/objects/a6/690261aaa341b065beea15f51e56e89a8a7c7a\n",
            "pygcn/.git/objects/8e/\n",
            "pygcn/.git/objects/8e/42cae27e2447364c2f9ba4899b5351e2575a9e\n",
            "pygcn/.git/objects/29/\n",
            "pygcn/.git/objects/29/9b90de321a8747e96dbd9e1c44bc30f6788986\n",
            "pygcn/.git/objects/16/\n",
            "pygcn/.git/objects/16/00b5b748b3976413d1e307540ccc62605b4d6d\n",
            "pygcn/.git/objects/5b/\n",
            "pygcn/.git/objects/5b/d4dee5c0205e843641c58b371a1e9b7bd531eb\n",
            "pygcn/.git/objects/6a/\n",
            "pygcn/.git/objects/6a/bbb6bf03ccb396f259e922f044ac6e94838233\n",
            "pygcn/.git/objects/6a/5f6016c09b777cc921559cf32e0aea388d7dd0\n",
            "pygcn/.git/objects/c4/\n",
            "pygcn/.git/objects/c4/7dc5fef0f2907aeb9725311d173ed7b5861797\n",
            "pygcn/.git/objects/c4/184542d51b6cb7882a88dfc5f032d0989dab6b\n",
            "pygcn/.git/objects/f9/\n",
            "pygcn/.git/objects/f9/42002d3ab5336326d6fb21ab7cf03b23f7a415\n",
            "pygcn/.git/objects/b8/\n",
            "pygcn/.git/objects/b8/3580f37b4129fa2bac067447874e0d2d1fa7d5\n",
            "pygcn/.git/objects/pack/\n",
            "pygcn/.git/objects/d1/\n",
            "pygcn/.git/objects/d1/911079c0ba27ae939d8384cd1b21dbb6e07563\n",
            "pygcn/.git/objects/dd/\n",
            "pygcn/.git/objects/dd/0e3722ffcb0a956d1a7d24cba095e0c05dbed5\n",
            "pygcn/.git/objects/2e/\n",
            "pygcn/.git/objects/2e/b9fcad6cbb9ba59504f1892e1fa1baef94a75e\n",
            "pygcn/.git/objects/25/\n",
            "pygcn/.git/objects/25/3f46d0772118219aeac6437bd7eaa703006308\n",
            "pygcn/.git/objects/e2/\n",
            "pygcn/.git/objects/e2/7317f9dfb355a30d1ac76560206feeb1b84e1d\n",
            "pygcn/.git/objects/23/\n",
            "pygcn/.git/objects/23/205398c433481a3b8c504e95e9f15adb47fe13\n",
            "pygcn/.git/objects/23/89625f5aeec73c083643d9ab5a0518824f0d54\n",
            "pygcn/.git/objects/23/31e711d3c82593af683b5c4d3f42627574f414\n",
            "pygcn/.git/logs/\n",
            "pygcn/.git/logs/HEAD\n",
            "pygcn/.git/logs/refs/\n",
            "pygcn/.git/logs/refs/remotes/\n",
            "pygcn/.git/logs/refs/remotes/origin/\n",
            "pygcn/.git/logs/refs/remotes/origin/HEAD\n",
            "pygcn/.git/logs/refs/heads/\n",
            "pygcn/.git/logs/refs/heads/master\n",
            "pygcn/.git/packed-refs\n",
            "pygcn/.git/index\n",
            "pygcn/.git/description\n",
            "pygcn/.git/HEAD\n",
            "pygcn/.git/info/\n",
            "pygcn/.git/info/exclude\n",
            "pygcn/.git/branches/\n",
            "pygcn/.git/config\n",
            "pygcn/.git/hooks/\n",
            "pygcn/.git/hooks/update.sample\n",
            "pygcn/.git/hooks/commit-msg.sample\n",
            "pygcn/.git/hooks/pre-push.sample\n",
            "pygcn/.git/hooks/pre-rebase.sample\n",
            "pygcn/.git/hooks/applypatch-msg.sample\n",
            "pygcn/.git/hooks/post-update.sample\n",
            "pygcn/.git/hooks/pre-applypatch.sample\n",
            "pygcn/.git/hooks/pre-commit.sample\n",
            "pygcn/.git/hooks/prepare-commit-msg.sample\n",
            "pygcn/.git/hooks/pre-receive.sample\n",
            "pygcn/.git/hooks/fsmonitor-watchman.sample\n",
            "pygcn/.git/refs/\n",
            "pygcn/.git/refs/remotes/\n",
            "pygcn/.git/refs/remotes/origin/\n",
            "pygcn/.git/refs/remotes/origin/HEAD\n",
            "pygcn/.git/refs/heads/\n",
            "pygcn/.git/refs/heads/master\n",
            "pygcn/.git/refs/tags/\n",
            "pygcn/figure.png\n"
          ]
        }
      ]
    }
  ]
}